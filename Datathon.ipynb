{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Space Traffic Navigation and Analytics\n",
        "Improving the **Orbital Heatmap Visualization** project my team worked on for the **Women in Data - Space Aware Datathon 2025** -\n",
        "1.   **[Payton Maurer](https://www.linkedin.com/in/paytonmaurer/)** - Sourced and Processed UDL Bulk Data. Set up Analytics in Google Looker Studio adding integrations with Google BigQuery.\n",
        "2.   **[Debisree Ray](https://www.linkedin.com/in/debisree-ray-ph-d-82241355/)** - Processed UDL Bulk Data. Generated real-time and historic data heatmap in a Gradio app interface.\n",
        "3.   **Varshitha Venkatesh** - Processed UDL Bulk data and designed a web interface to display processed data in world map and 3D globe view with metric filters.\n",
        "\n",
        "\n",
        "As an extension, for my portfolio project, I am reimplmenting the entire pipeline end-to-end. This notebook:\n",
        "* Extracts the sourced UDL Bulk Data from **Google Drive**.\n",
        "* Normalizes it.\n",
        "* Runs conversion scripts to geojson format for website visualization with **THREE.js.**\n",
        "* Adds the dataframes to **Google Sheets** by integrating with **Google BigQuery** for **Google Looker Studio** analytics.\n",
        "\n",
        "Live Site Accessible Here - https://varshivenkatesh.github.io/space-traffic-nav/"
      ],
      "metadata": {
        "id": "3v2E9Fkluaro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount Google Drive"
      ],
      "metadata": {
        "id": "_EbJyXkToXcB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BOLIU4XYoKbO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2821da8a-4ef7-4200-eeb7-914fbee4509c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✅ Drive mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# 1.1 Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Quick sanity check\n",
        "import os\n",
        "assert os.path.exists(\"/content/drive/MyDrive\"), \"Drive not mounted correctly.\"\n",
        "print(\"✅ Drive mounted at /content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Project Dir\n",
        "project_dir = \"/content/drive/MyDrive/Visualization/space-traffic-nav\"\n",
        "os.chdir(project_dir)"
      ],
      "metadata": {
        "id": "kJsnHxKWKgus"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract Data"
      ],
      "metadata": {
        "id": "R1aQyZ9Vp-EV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, zipfile\n",
        "\n",
        "# Data Dir\n",
        "data_dir = \"/content/drive/MyDrive/Visualization/space-traffic-nav/raw_zip\"\n",
        "os.chdir(data_dir)\n",
        "\n",
        "# Create folder to extract\n",
        "extract_dir = os.path.join(project_dir, \"extracted\")\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "# Unzip all files\n",
        "for file in os.listdir(data_dir):\n",
        "    if file.endswith(\".zip\"):\n",
        "        with zipfile.ZipFile(os.path.join(data_dir, file), 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_dir)\n",
        "\n",
        "print(\"✅ All files extracted to:\", extract_dir)"
      ],
      "metadata": {
        "id": "K3-0WLsgrNXQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e654d8f8-84cc-4601-cf81-6f7383d7ee9e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All files extracted to: /content/drive/MyDrive/Visualization/space-traffic-nav/extracted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Conjunction Data\n",
        "json_files_1 = [f for f in os.listdir(extract_dir) if f.endswith(\".json\")]\n",
        "with open(os.path.join(extract_dir, json_files_1[0])) as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "df1 = pd.json_normalize(data)  # flatten nested JSON 1\n",
        "print(df1.head())\n",
        "\n",
        "json_files_2 = [f for f in os.listdir(extract_dir) if f.endswith(\".json\")]\n",
        "with open(os.path.join(extract_dir, json_files_2[1])) as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "df2 = pd.json_normalize(data)  # flatten nested JSON 2\n",
        "print(df2.head())\n",
        "\n",
        "json_files_3 = [f for f in os.listdir(extract_dir) if f.endswith(\".json\")]\n",
        "with open(os.path.join(extract_dir, json_files_3[2])) as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "df3 = pd.json_normalize(data)  # flatten nested JSON 3\n",
        "print(df3.head())\n",
        "\n",
        "json_files_4 = [f for f in os.listdir(extract_dir) if f.endswith(\".json\")]\n",
        "with open(os.path.join(extract_dir, json_files_4[3])) as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "df4 = pd.json_normalize(data)  # flatten nested JSON 4\n",
        "print(df4.head())\n",
        "\n",
        "json_files_5 = [f for f in os.listdir(extract_dir) if f.endswith(\".json\")]\n",
        "with open(os.path.join(extract_dir, json_files_5[4])) as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "df5 = pd.json_normalize(data)  # flatten nested JSON 5\n",
        "print(df5.head())\n",
        "\n",
        "json_files_6 = [f for f in os.listdir(extract_dir) if f.endswith(\".json\")]\n",
        "with open(os.path.join(extract_dir, json_files_6[5])) as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "df6 = pd.json_normalize(data)  # flatten nested JSON 6\n",
        "print(df6.head())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_S9uvB7Tur82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ELSET Data\n",
        "json_files_7 = [f for f in os.listdir(extract_dir) if f.endswith(\".json\")]\n",
        "with open(os.path.join(extract_dir, json_files_7[6])) as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "df7 = pd.json_normalize(data)  # flatten nested JSON 7\n",
        "print(df7.head())\n",
        "\n",
        "# SGI Data\n",
        "json_files_8 = [f for f in os.listdir(extract_dir) if f.endswith(\".json\")]\n",
        "with open(os.path.join(extract_dir, json_files_8[7])) as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "df8 = pd.json_normalize(data)  # flatten nested JSON 1\n",
        "print(df8.head())\n",
        "\n",
        "# StateVector Data\n",
        "json_files_9 = [f for f in os.listdir(extract_dir) if f.endswith(\".json\")]\n",
        "with open(os.path.join(extract_dir, json_files_9[8])) as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "df9 = pd.json_normalize(data)  # flatten nested JSON 1\n",
        "print(df9.head())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WcYmGHFELeum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Push Data to Sheets - just an option"
      ],
      "metadata": {
        "id": "KOn6Uc_iMSFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sheets API Client\n",
        "!pip install --quiet gspread gspread_dataframe"
      ],
      "metadata": {
        "id": "hHAWYaLMvubW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Authentication\n",
        "# Authenticate user\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Import packages\n",
        "import gspread\n",
        "from gspread_dataframe import set_with_dataframe\n",
        "from google.auth import default\n",
        "\n",
        "# Get creds and authorize\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "# Create new sheets\n",
        "sh = gc.create(\"Conjunction_Raw_Data_Pipeline\")\n",
        "worksheet1 = sh.get_worksheet(0)\n",
        "worksheet2 = sh.add_worksheet(title=\"Sheet2\", rows=\"1000\", cols=\"20\")\n",
        "worksheet3 = sh.add_worksheet(title=\"Sheet3\", rows=\"1000\", cols=\"20\")\n",
        "worksheet4 = sh.add_worksheet(title=\"Sheet4\", rows=\"1000\", cols=\"20\")\n",
        "worksheet5 = sh.add_worksheet(title=\"Sheet5\", rows=\"1000\", cols=\"20\")\n",
        "worksheet6 = sh.add_worksheet(title=\"Sheet6\", rows=\"1000\", cols=\"20\")\n",
        "\n",
        "sh1 = gc.create(\"ELSET_Raw_Data_Pipeline\")\n",
        "worksheet7 = sh.get_worksheet(0)\n",
        "\n",
        "sh2 = gc.create(\"SGI_Raw_Data_Pipeline\")\n",
        "worksheet8 = sh.get_worksheet(0)\n",
        "\n",
        "sh3 = gc.create(\"StateVector_Raw_Data_Pipeline\")\n",
        "worksheet9 = sh.get_worksheet(0)"
      ],
      "metadata": {
        "id": "6MHUYW7YvyTp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Push data to google sheets\n",
        "\n",
        "# conjunction\n",
        "set_with_dataframe(worksheet1, df1)\n",
        "set_with_dataframe(worksheet2, df2)\n",
        "set_with_dataframe(worksheet3, df3)\n",
        "set_with_dataframe(worksheet4, df4)\n",
        "set_with_dataframe(worksheet5, df5)\n",
        "set_with_dataframe(worksheet6, df6)\n",
        "\n",
        "# elset\n",
        "set_with_dataframe(worksheet7, df7)\n",
        "\n",
        "# sgi\n",
        "set_with_dataframe(worksheet8, df8)\n",
        "\n",
        "# statevector\n",
        "set_with_dataframe(worksheet9, df9)\n",
        "\n",
        "print(\"✅ Data pushed to Google Sheets\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7KwfZPMwiOq",
        "outputId": "5e2a5868-50d9-482e-bbe6-3dc338820df5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Data pushed to Google Sheets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combining All Data for Visualization - **conjunctions.json**\n"
      ],
      "metadata": {
        "id": "OeXffZLCCgWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import glob\n",
        "\n",
        "# Path to your JSON files - mention the metric containing multiple files\n",
        "path = \"/content/drive/MyDrive/Visualization/space-traffic-nav/extracted/*conjunction*.json\"\n",
        "\n",
        "# Load all JSON files into a list of DataFrames\n",
        "dfs = []\n",
        "for file in glob.glob(path):\n",
        "    with open(file, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    # Normalize JSON into a dataframe\n",
        "    df = pd.json_normalize(data)\n",
        "    print(f\"{file} → {df.shape}\")  # see rows/cols per file\n",
        "    dfs.append(df)\n",
        "\n",
        "# Combine all into one dataframe\n",
        "conjunctions = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# If you want to keep it in Drive:\n",
        "conjunctions.to_json(\"/content/drive/MyDrive/Visualization/space-traffic-nav/conjunctions.json\",\n",
        "                     orient=\"records\", indent=2)\n",
        "print(\"Combined Shape:\", conjunctions.shape)\n",
        "\n",
        "# Preview columns\n",
        "print(conjunctions.head())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "J14JN_OY1sPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "project_dir = \"/content/drive/MyDrive/Visualization/space-traffic-nav\"\n",
        "os.chdir(project_dir)\n",
        "\n",
        "# Conjunctions Combined Data\n",
        "json_files = [f for f in os.listdir(extract_dir) if f.endswith(\".json\")]\n",
        "with open(os.path.join(extract_dir, json_files[0])) as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "df = pd.json_normalize(data)  # flatten nested JSON 1\n",
        "print(df.head())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "VunRg_xmZaMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handling Data in BigQuery for Looker Studio Analytics**\n",
        "\n",
        "Better than handling with Google Sheets since there are 50000 data points in each JSON file"
      ],
      "metadata": {
        "id": "ZhlQ3FpOUlJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "EJmqvLSGVnSZ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet pandas-gbq google-cloud-bigquery"
      ],
      "metadata": {
        "id": "iSMdPDzuW8vM"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas_gbq import to_gbq\n",
        "\n",
        "project_id = \"<project id>\"\n",
        "dataset_id = \"<dataset name>\"   # create in BigQuery\n",
        "\n",
        "# conjunctions\n",
        "to_gbq(df, f\"{dataset_id}.conjunctions\", project_id=project_id, if_exists=\"replace\")\n",
        "\n",
        "# elset\n",
        "to_gbq(df7, f\"{dataset_id}.elset\", project_id=project_id, if_exists=\"replace\")\n",
        "\n",
        "# sgi\n",
        "to_gbq(df8, f\"{dataset_id}.sgi\", project_id=project_id, if_exists=\"replace\")\n",
        "\n",
        "# statevector\n",
        "to_gbq(df9, f\"{dataset_id}.statevector\", project_id=project_id, if_exists=\"replace\")\n",
        "\n",
        "print(\"✅ Data pushed to BigQuery\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Xzeemo9W_FN",
        "outputId": "b401f753-5cce-4705-c4ec-0615e777a196"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 9238.56it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 5121.25it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 10922.67it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 12018.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Data pushed to BigQuery\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "JSON → GeoJSON for 2D/3D Visualization"
      ],
      "metadata": {
        "id": "ZVcq_lxvC3rJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sgp4"
      ],
      "metadata": {
        "id": "f9QMEZVJhDau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Project Directory\n",
        "project_dir = \"/content/drive/MyDrive/Visualization\"\n",
        "os.chdir(project_dir)"
      ],
      "metadata": {
        "id": "ODHGQ5aUgJHJ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python geojson.py"
      ],
      "metadata": {
        "id": "ZfB8FP19fwI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pushing Changes to Git to update info for the Website!"
      ],
      "metadata": {
        "id": "a609Rk_Pnk43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = \"/content/drive/MyDrive/Visualization\"\n",
        "os.chdir(base_dir)"
      ],
      "metadata": {
        "id": "Rtncpch-8Bhv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Git if not already available\n",
        "!apt-get install git -y\n",
        "\n",
        "# Configure Git (set your GitHub email and username here)\n",
        "!git config --global user.email \"<email>\"\n",
        "!git config --global user.name \"<user>\"\n",
        "\n",
        "# Clone your repository into Colab\n",
        "!git clone <repo>.git"
      ],
      "metadata": {
        "id": "eKJoWi8kk09g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git add .\n",
        "!git commit -m \"Demo Commit\""
      ],
      "metadata": {
        "id": "ho15UTGZxkNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Go into your repo folder\n",
        "%cd /content/drive/MyDrive/<repo folder>\n",
        "\n",
        "!git remote set-url origin https://<user>:<token>@github.com/<user>/<repo>.git\n",
        "\n",
        "# Use token inline to push (replace <YOUR_TOKEN>)\n",
        "!git push origin main --force\n"
      ],
      "metadata": {
        "id": "QwRKJwVRqFww"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}